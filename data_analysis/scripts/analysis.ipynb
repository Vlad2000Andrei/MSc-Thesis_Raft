{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import path, walk, listdir\n",
    "from os.path import isfile\n",
    "from statistics import fmean\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNTIME_SECONDS = 3600\n",
    "\n",
    "DIR_CLASSIC = \"../bench_data/classic/\"\n",
    "DIR_MODIFIED = \"../bench_data/modified/\"\n",
    "\n",
    "REGEX_NODES = r\"\\d+_nodes\"\n",
    "REGEX_VERSION = r\"(modified)|(classic)\"\n",
    "REGEX_DURATION = r\"\\d+_minutes\"\n",
    "REGEX_RUN = r\"run_\\d+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_csvs(rootpath : str) -> list[str]:\n",
    "    csvs = []\n",
    "    for root, dirs, files in walk(rootpath):\n",
    "        for dir in dirs:\n",
    "            dir_path = path.join(root, dir)\n",
    "            csvs = csvs + [path.normpath(path.abspath(path.join(root, dir, file))) for file in listdir(dir_path) if file.endswith('csv')]\n",
    "    return csvs\n",
    "\n",
    "def find_all_csvs_and_join(rootpaths : list[str]) -> list[str]:\n",
    "    csvs = []\n",
    "    for path in rootpaths:\n",
    "        csvs = csvs + find_all_csvs(path)\n",
    "    return csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = find_all_csvs(\"..\\\\bench_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv(path : str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df.leaderId != -1] # Remove the initial dummy entries\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_node_count(df : pd.DataFrame) -> int:\n",
    "    return len(df[\"serverId\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quorum_size(df : pd.DataFrame) -> int:\n",
    "    return find_node_count(df) // 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_count(file : str) -> int:\n",
    "    num, _ = re.search(REGEX_NODES, file).group().split(\"_\", maxsplit=1)\n",
    "    return int(num)\n",
    "\n",
    "def get_duration_minutes(file : str) -> int:\n",
    "    num, _ = re.search(REGEX_DURATION, file).group().split(\"_\", maxsplit=1)\n",
    "    return int(num)\n",
    "\n",
    "def get_algorithm_type(file : str) -> str:\n",
    "    return re.search(REGEX_VERSION, file).group()\n",
    "\n",
    "def get_run_id(file : str) -> str:\n",
    "    return re.search(REGEX_RUN, file).group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work per Leader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Committed entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_leader_commit_counts(df : pd.DataFrame, title=None):\n",
    "    leaders = df[\"leaderId\"].value_counts()\n",
    "    leaders = leaders / len(df)\n",
    "    # plt.bar(leaders.index, leaders)\n",
    "    sns.barplot(leaders)\n",
    "    plt.ylim(0,1)\n",
    "    if not title is None:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in FILES:\n",
    "    algo = get_algorithm_type(file)\n",
    "    nodes = get_node_count(file)\n",
    "    minutes = get_duration_minutes(file)\n",
    "    run = get_run_id(file)\n",
    "    plot_leader_commit_counts(open_csv(file), f\"{algo} {run}: {nodes} nodes for {minutes} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replicated_count(df : pd.DataFrame, node_count : int) -> int:\n",
    "    entry_counts = df.groupby('serverId').size()\n",
    "    committed = []\n",
    "    for count in entry_counts:\n",
    "        replicated_count = sum(map(lambda x: x >= count, entry_counts))\n",
    "        if replicated_count >= node_count:\n",
    "            committed.append(count)\n",
    "    return max(committed) if len(committed) > 0 else 0\n",
    "\n",
    "def find_committed_count(df : pd.DataFrame) -> int:\n",
    "    return find_replicated_count(df, find_quorum_size(df))\n",
    "\n",
    "def find_fully_replicated_count(df : pd.DataFrame) -> int:\n",
    "    return find_replicated_count(df, find_node_count(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_logs_by(df : pd.DataFrame, column : str) -> pd.DataFrame:\n",
    "    logs : list[pd.DataFrame] = []\n",
    "    for val in df[column].unique():\n",
    "        logs.append(df[df[column] == val])\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replication_delays(df : pd.DataFrame, num_nodes : int):\n",
    "    logs = split_logs_by(df, \"serverId\")\n",
    "\n",
    "    last_replicated = find_replicated_count(df, num_nodes)\n",
    "    \n",
    "    delays = []\n",
    "\n",
    "    for i in range(last_replicated):\n",
    "        creation_time = min([log.iloc[i][\"creationTime\"] for log in logs if len(log) > i])\n",
    "        storage_times = [log.iloc[i][\"storageTime\"] for log in logs if len(log) > i]\n",
    "        \n",
    "        if len(storage_times) < num_nodes:\n",
    "            delays.append(None)\n",
    "            print(f\"None at {i}. Need {num_nodes}, have {len(storage_times)}\")\n",
    "        else:\n",
    "            storage_times.sort()\n",
    "            delays.append(storage_times[num_nodes - 1] - creation_time)\n",
    "    return delays\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(paths : list[str], csv : str) -> pd.DataFrame:\n",
    "    unprocessed_paths = []\n",
    "    unprocessed_paths = paths\n",
    "    existing_df = None\n",
    "\n",
    "    algorithms = []\n",
    "    node_counts = []\n",
    "    durations_minutes = []\n",
    "    run_ids = []\n",
    "    dataframes = []\n",
    "    full_replication_delays = []\n",
    "    commit_delays = []\n",
    "    committed_counts = []\n",
    "    fully_replicated_counts = []\n",
    "    committed_rates = []\n",
    "    fully_replicated_rates = []\n",
    "    longest_logs = []\n",
    "\n",
    "\n",
    "    for file in tqdm(unprocessed_paths):\n",
    "        df = open_csv(file)\n",
    "\n",
    "        algorithms.append(get_algorithm_type(file))\n",
    "        node_counts.append(get_node_count(file))\n",
    "        durations_minutes.append(get_duration_minutes(file))\n",
    "        run_ids.append(get_run_id(file))\n",
    "        dataframes.append(df)\n",
    "        full_replication_delays.append(find_replication_delays(df, find_node_count(df)))\n",
    "        commit_delays.append(find_replication_delays(df, find_quorum_size(df)))\n",
    "        committed_counts.append(find_committed_count(df))\n",
    "        fully_replicated_counts.append(find_fully_replicated_count(df))\n",
    "        longest_logs.append(find_replicated_count(df, 1))\n",
    "\n",
    "    fully_replicated_rates = np.array(fully_replicated_counts) / RUNTIME_SECONDS\n",
    "    committed_rates = np.array(committed_counts) / RUNTIME_SECONDS\n",
    "\n",
    "    columns = {\"path\": paths, \n",
    "               \"algorithm\": algorithms, \n",
    "               \"nodeCount\": node_counts, \n",
    "               \"duration\": durations_minutes, \n",
    "               \"runId\": run_ids, \n",
    "               \"fullReplicationDelays\": full_replication_delays,\n",
    "               \"commitDelays\": commit_delays,\n",
    "               \"fullReplicationCount\": fully_replicated_counts, \n",
    "               \"commitCount\": committed_counts, \n",
    "               \"fullReplicationRate\": fully_replicated_rates,\n",
    "               \"commitRate\": committed_rates,\n",
    "               \"longest_logs\": longest_logs\n",
    "               }\n",
    "    new_entries = pd.DataFrame(columns)\n",
    "\n",
    "    if existing_df is None:\n",
    "       new_entries = new_entries.sort_values(by=[\"algorithm\", \"nodeCount\", \"runId\"])\n",
    "    #    new_entries.to_csv(csv)\n",
    "       return new_entries\n",
    "    else:\n",
    "        result = pd.concat([existing_df, new_entries]).sort_values(by=[\"algorithm\", \"nodeCount\", \"runId\"]).reindex()\n",
    "        # result.to_csv(csv)\n",
    "        return result\n",
    "\n",
    "\n",
    "paths = find_all_csvs(\"../bench_data/\")\n",
    "processed_path = \"../bench_data/processed.csv\"\n",
    "DATA = process_data(paths, processed_path)\n",
    "print(DATA.dtypes)\n",
    "display(DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in FILES:\n",
    "    df = open_csv(file)\n",
    "    print(f\"{file.split(\"bench_data\", maxsplit=1)[1]}:\\t committed {find_committed_count(df)}/{find_replicated_count(df, 1)},\\t fully replicated {find_fully_replicated_count(df)}/{find_replicated_count(df, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_from_all(data : pd.DataFrame, colname : str, algo : str = None, nodes : str = None):\n",
    "    filtered = data\n",
    "    if not algo is None:\n",
    "        filtered = filtered[filtered[\"algorithm\"] == algo]\n",
    "    if not nodes is None:\n",
    "        filtered = filtered[filtered[\"nodeCount\"] == nodes]\n",
    "    \n",
    "    if colname in [\"fullReplicationDelays\", \"commitDelays\"]:\n",
    "        nested_lists = filtered[colname].tolist()\n",
    "        return sum(nested_lists, start=[])\n",
    "    else:\n",
    "        return filtered[colname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall\n",
    "modified_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"modified\")\n",
    "classic_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"classic\")\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.ecdfplot(modified_commit_delays, log_scale=True, label=\"modified\")\n",
    "sns.ecdfplot(classic_commit_delays, log_scale=True, label=\"classic\")\n",
    "plt.title(\"Overall commit latency of classic vs modified\")\n",
    "plt.xlabel(\"Latency [milliseconds]\")\n",
    "plt.xlim(0.0000001, 1000)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.gca().xaxis.minorticks_on()\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"modified\", nodes=3)\n",
    "classic_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"classic\", nodes=3)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.ecdfplot(modified_commit_delays, log_scale=True, label=\"modified\")\n",
    "sns.ecdfplot(classic_commit_delays, log_scale=True, label=\"classic\")\n",
    "plt.title(\"Overall commit latency of classic vs modified\")\n",
    "plt.xlabel(\"Latency [milliseconds]\")\n",
    "plt.xlim(0.0000001, 1000)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.gca().xaxis.minorticks_on()\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"modified\", nodes=5)\n",
    "classic_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"classic\", nodes=5)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.ecdfplot(modified_commit_delays, log_scale=True, label=\"modified\")\n",
    "sns.ecdfplot(classic_commit_delays, log_scale=True, label=\"classic\")\n",
    "plt.title(\"Overall commit latency of classic vs modified\")\n",
    "plt.xlabel(\"Latency [milliseconds]\")\n",
    "plt.xlim(0.0000001, 1000)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.gca().xaxis.minorticks_on()\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"modified\", nodes=7)\n",
    "classic_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"classic\", nodes=7)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.ecdfplot(modified_commit_delays, log_scale=True, label=\"modified\")\n",
    "sns.ecdfplot(classic_commit_delays, log_scale=True, label=\"classic\")\n",
    "plt.title(\"Overall commit latency of classic vs modified\")\n",
    "plt.xlabel(\"Latency [milliseconds]\")\n",
    "plt.xlim(0.0000001, 1000)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.gca().xaxis.minorticks_on()\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"modified\", nodes=9)\n",
    "classic_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"classic\", nodes=9)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.ecdfplot(modified_commit_delays, log_scale=True, label=\"modified\")\n",
    "sns.ecdfplot(classic_commit_delays, log_scale=True, label=\"classic\")\n",
    "plt.title(\"Overall commit latency of classic vs modified\")\n",
    "plt.xlabel(\"Latency [milliseconds]\")\n",
    "plt.xlim(0.0000001, 1000)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.gca().xaxis.minorticks_on()\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall\n",
    "modified_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"modified\", nodes=15)\n",
    "classic_commit_delays = get_col_from_all(DATA, \"commitDelays\", algo=\"classic\", nodes=15)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.ecdfplot(modified_commit_delays, log_scale=True, label=\"modified\")\n",
    "sns.ecdfplot(classic_commit_delays, log_scale=True, label=\"classic\")\n",
    "plt.title(\"Overall commit latency of classic vs modified\")\n",
    "plt.xlabel(\"Latency [milliseconds]\")\n",
    "plt.xlim(0.0000001, 1000)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.gca().xaxis.minorticks_on()\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.gca()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
